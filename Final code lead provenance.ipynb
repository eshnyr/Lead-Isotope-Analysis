{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f826b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f66246",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = Path(input(\"Enter path to your data file (.xlsx): \")).expanduser()\n",
    "new_data = pd.read_excel(\n",
    "    data_file,\n",
    "    sheet_name=\"Directly usable data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e054718",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_data.dropna(axis =0, subset=['corrected Region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0894a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_regions = new_data['corrected Region'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cbebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_list = new_data['corrected Region'].value_counts()\n",
    "regions_list_df = regions_list.to_frame()\n",
    "regions_list_df = regions_list_df[regions_list_df['corrected Region'] < 5]\n",
    "regions_list_df = regions_list_df.reset_index()\n",
    "\n",
    "regions_to_remove = regions_list_df['index'].unique()\n",
    "\n",
    "new_data = new_data[~new_data['corrected Region'].isin(regions_to_remove)]\n",
    "\n",
    "new_data['corrected Region'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13611e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = new_data[\"corrected Region\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ffc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa01b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_10percent_outliers_marker(df,regions,file_name):\n",
    "    new_df = pd.DataFrame()\n",
    "    for i in range(len(regions)):\n",
    "        region_df = df[df['corrected Region'] == regions[i]].copy()\n",
    "        \n",
    "        min_num_of_neighbours = math.ceil(len(region_df['206Pb/204Pb']) * (10/100))# calculate 10%\n",
    "        if (min_num_of_neighbours ==1):\n",
    "            min_num_of_neighbours = 2\n",
    "            \n",
    "        selected_columns = ['206Pb/204Pb', '207Pb/204Pb', '208Pb/204Pb']\n",
    "        region_df_selected = region_df[selected_columns].copy()  # Create a copy to avoid modifying the original DataFrame\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        region_df_scaled = scaler.fit_transform(region_df_selected)\n",
    "        \n",
    "        region_df_scaled = pd.DataFrame(region_df_scaled, columns=selected_columns)\n",
    "        \n",
    "        dbscan = DBSCAN(eps=0.2, min_samples=min_num_of_neighbours).fit(region_df_scaled) #trained on 10% min_samples\n",
    "        region_df['outliers'] = dbscan.labels_\n",
    "        region_df['group'] = region_df['outliers'].map(lambda x: regions[i] + ' ' + str(x))\n",
    "        \n",
    "        new_df = pd.concat([region_df,new_df],axis=0)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "        \n",
    "        #output_dir = '/Users/new/Desktop/Thesis/outputs/Outliers/DBscan_outliers/'\n",
    "        #file_path = os.path.join(output_dir, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42806099",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after_dbscan = dbscan_10percent_outliers_marker(new_data,regions,'File name') # In new df column \"outliers\" added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49e57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after_dbscan.to_excel('enter_your_direction_with_outliers.xlsx', index=False) #save with outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe1d01",
   "metadata": {},
   "source": [
    "# Remove outliers and group regions with less then 20 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0ad9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after_dbscan_no_outliers = df_after_dbscan[df_after_dbscan['outliers']!= -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc6790",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_region = df_after_dbscan_no_outliers.groupby('corrected Region')['corrected Region'].transform('count')\n",
    "regions_with_few_samples = samples_per_region <= 20\n",
    "\n",
    "# Step 3: Assign 1 to 'group' column for samples from regions with 20 or fewer samples\n",
    "df_after_dbscan_no_outliers.loc[regions_with_few_samples, 'group'] = df_after_dbscan_no_outliers.loc[regions_with_few_samples, 'corrected Region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28bd7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after_dbscan_no_outliers.to_excel('enter_your_direction_no_outliers.xlsx',index=False) #save without outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bd4f7a",
   "metadata": {},
   "source": [
    "# SMOTE + Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdd0378",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_clustered = pd.read_excel('enter_your_direction_no_outliers.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7361f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_arr = [r for r in new_clustered['group'].unique() if pd.notna(r)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c1e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(df,region_name):\n",
    "    \n",
    "    df['class'] = np.where(df['group']==region_name, 1, 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a3e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_generator(df):#k_neighbors=3 changed to 2\n",
    "    num_of_ones = df['class'].sum()\n",
    "\n",
    "# Determine k_neighbors based on the count of \"1\"\n",
    "    if num_of_ones < 2:\n",
    "        # Simply return the original X, y untouched (or you could return None)\n",
    "        X = df[[\"206Pb/204Pb\",\"207Pb/204Pb\",\"208Pb/204Pb\"]]\n",
    "        y = df['class']\n",
    "        return X, y\n",
    "    \n",
    "    if num_of_ones == 2:\n",
    "        k_neighbors = 1\n",
    "    elif num_of_ones == 3:\n",
    "        k_neighbors = 2\n",
    "    elif num_of_ones == 4:\n",
    "        k_neighbors = 3\n",
    "    elif num_of_ones == 5:\n",
    "        k_neighbors = 4\n",
    "    elif num_of_ones == 6:\n",
    "        k_neighbors = 5\n",
    "    elif num_of_ones == 7:\n",
    "        k_neighbors = 6\n",
    "    elif num_of_ones == 8:\n",
    "        k_neighbors = 7\n",
    "    elif num_of_ones == 9:\n",
    "        k_neighbors = 8\n",
    "    elif num_of_ones == 10:\n",
    "        k_neighbors = 9\n",
    "    elif num_of_ones > 10:\n",
    "        k_neighbors = 10\n",
    "        \n",
    "    oversample = SMOTE(k_neighbors=k_neighbors)\n",
    "    X = df[[\"206Pb/204Pb\",\"207Pb/204Pb\",\"208Pb/204Pb\"]]\n",
    "    y = df['class']\n",
    "    \n",
    "    X_resampled, y_resampled = oversample.fit_resample(X, y)\n",
    "    return X_resampled,y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f384c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_binom_class(df,region_name):\n",
    "    \n",
    "    encoded_df = encoder(df,region_name)\n",
    "    X,y = smote_generator(encoded_df)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "    # Skip if only one class present\n",
    "    if y_train.nunique() < 2:\n",
    "        print(f\"Skipping region {region_name!r}: only one class in training data\")\n",
    "        return None\n",
    "\n",
    "    # 4) Train & save\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    model.save_model(f\"{region_name}_xgb_model.json\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e9845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(X,y):\n",
    "    scoring = [\"accuracy\",\"precision\",\"recall\",\"f1\"]\n",
    "    xgb = XGBClassifier()\n",
    "    \n",
    "    results = cross_validate(xgb,X,y,cv=10,scoring=scoring)\n",
    "    \n",
    "    results['fit_time'] = np.mean(results['fit_time'])\n",
    "    results['score_time'] = np.mean(results['score_time'])\n",
    "    results['test_accuracy'] = np.mean(results['test_accuracy'])\n",
    "    results['test_precision'] = np.mean(results['test_precision'])\n",
    "    results['test_recall'] = np.mean(results['test_recall'])\n",
    "    results['test_f1'] = np.mean(results['test_f1'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e51d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_class(df,region_name):\n",
    "    \n",
    "    encoded_df = encoder(df,region_name)\n",
    "    X,y = smote_generator(encoded_df)\n",
    "    results = validate(X,y)\n",
    "    #model = train_class(X,y)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e895a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR CLUSTERS\n",
    "models = {}\n",
    "\n",
    "for cluster in regions_arr:\n",
    "    model = train_binom_class(new_clustered,cluster)\n",
    "    models[cluster] = model\n",
    "    if model is not None:\n",
    "        models[cluster] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330519e5",
   "metadata": {},
   "source": [
    "# Predict new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8215bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_excel(\"your_data_to_predict.xlsx\")\n",
    "# The data should be organised in an Excel file with column headings: '206Pb/204Pb', '207Pb/204Pb', '208Pb/204Pb'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b739f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prediction function processes one sample at a time.\n",
    "# To predict the first row, use test_data.head(1).\n",
    "# To predict the second row, use test_data.iloc[1:2].\n",
    "# For the third row, use test_data.iloc[2:3], and so on.\n",
    "\n",
    "probabilities = {}\n",
    "\n",
    "for clf in models:\n",
    "    model = models[clf]\n",
    "    proba = model.predict_proba(test_data.head(1)) #for 2nd line use test_data.iloc[1:2]\n",
    "    probabilities[clf] = proba\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46aad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for region_name, values in probabilities.items():\n",
    "    class_1 = values[0][1]    \n",
    "    class_0 = values[0][0]\n",
    "\n",
    "    data.append({\n",
    "        'region_name': region_name,\n",
    "        'class_0': class_0,\n",
    "        'class_1': class_1,\n",
    "        'type': 'float32'\n",
    "    })\n",
    "\n",
    "df_all = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d37c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.round(3)\n",
    "df_all.sort_values(\"class_1\",ascending = False)\n",
    "\n",
    "#The results show the probabilities for a single test sample, sorted from highest to lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe91a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
